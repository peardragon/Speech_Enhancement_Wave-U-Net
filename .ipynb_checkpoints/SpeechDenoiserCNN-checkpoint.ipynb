{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VoyZ5yLRaAeI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ah7ISzvtEmur"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25408/4282809631.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import scipy\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import zipfile\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBZyUMpobR-Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zu2lgR8lEmu0"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(999)\n",
    "np.random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGbkvQHxxrj_"
   },
   "outputs": [],
   "source": [
    "!wget 'cdn.daitan.com/dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51o6Ze9hxrkE"
   },
   "outputs": [],
   "source": [
    "dataset_file_name = './dataset.zip'\n",
    "with zipfile.ZipFile(dataset_file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d43UvhEqxrkJ"
   },
   "outputs": [],
   "source": [
    "path_to_dataset = \"./dataset/tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W36WF4mT1kGK"
   },
   "outputs": [],
   "source": [
    "# get training and validation tf record file names\n",
    "train_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'train_*'))\n",
    "val_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'val_*'))\n",
    "\n",
    "# suffle the file names for training\n",
    "np.random.shuffle(train_tfrecords_filenames)\n",
    "print(\"Training file names: \", train_tfrecords_filenames)\n",
    "print(\"Validation file names: \", val_tfrecords_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKZtPoLMEmvJ"
   },
   "outputs": [],
   "source": [
    "windowLength = 256\n",
    "overlap      = round(0.25 * windowLength) # overlap of 75%\n",
    "ffTLength    = windowLength\n",
    "inputFs      = 48e3\n",
    "fs           = 16e3\n",
    "numFeatures  = ffTLength//2 + 1\n",
    "numSegments  = 8\n",
    "print(\"windowLength:\",windowLength)\n",
    "print(\"overlap:\",overlap)\n",
    "print(\"ffTLength:\",ffTLength)\n",
    "print(\"inputFs:\",inputFs)\n",
    "print(\"fs:\",fs)\n",
    "print(\"numFeatures:\",numFeatures)\n",
    "print(\"numSegments:\",numSegments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXpSGe8L0cl_"
   },
   "outputs": [],
   "source": [
    "mozilla_basepath = \"./dataset/en\"\n",
    "UrbanSound8K_basepath = './dataset/UrbanSound8K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzbdfIi-Lgk9"
   },
   "source": [
    "## Prepare Input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24kph3wJ3s5V"
   },
   "outputs": [],
   "source": [
    "def tf_record_parser(record):\n",
    "    keys_to_features = {\n",
    "        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n",
    "        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n",
    "    }\n",
    "\n",
    "    features = tf.io.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n",
    "    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n",
    "    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n",
    "\n",
    "    # reshape input and annotation images\n",
    "    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8, 1), name=\"noise_stft_mag_features\")\n",
    "    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129, 1, 1), name=\"clean_stft_magnitude\")\n",
    "    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n",
    "\n",
    "    return noise_stft_mag_features, clean_stft_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJXPGrgVTCbZ"
   },
   "source": [
    "## Create tf.Data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FF_A3YbZTCsj"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n",
    "train_dataset = train_dataset.map(tf_record_parser)\n",
    "train_dataset = train_dataset.shuffle(8192)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(512)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOuWPzQaTNDy"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\n",
    "test_dataset = test_dataset.map(tf_record_parser)\n",
    "test_dataset = test_dataset.repeat(1)\n",
    "test_dataset = test_dataset.batch(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEG5JofLSfRw"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OZFegXrSYle"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation\n",
    "from tensorflow.keras import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc2K0cfhPU5-"
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
    "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  if use_bn:\n",
    "    x = BatchNormalization()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFYyKoAVQYCz"
   },
   "outputs": [],
   "source": [
    "def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
    "  shortcut = x\n",
    "  in_channels = x.shape[-1]\n",
    "\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "  return shortcut + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlRQ3ngpZG1Y"
   },
   "outputs": [],
   "source": [
    "def build_model(l2_strength):\n",
    "  inputs = Input(shape=[numFeatures,numSegments,1])\n",
    "  x = inputs\n",
    "\n",
    "  # -----\n",
    "  x = tf.keras.layers.ZeroPadding2D(((4,4), (0,0)))(x)\n",
    "  x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(skip0)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # -----\n",
    "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(skip1)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # ----\n",
    "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  \n",
    "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # ----\n",
    "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = x + skip1\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # ----\n",
    "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = x + skip0\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # ----\n",
    "  x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n",
    "  x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss='mse', \n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNpHS4LuShxd"
   },
   "outputs": [],
   "source": [
    "model = build_model(l2_strength=0.0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rWOgsdwglFE"
   },
   "outputs": [],
   "source": [
    "# You might need to install the following dependencies: sudo apt install python-pydot python-pydot-ng graphviz\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OdmgHTp4_Ou"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BucSAwkHQj8Q"
   },
   "outputs": [],
   "source": [
    "baseline_val_loss = model.evaluate(test_dataset)[0]\n",
    "print(f\"Baseline accuracy {baseline_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocycFbP5-X0o"
   },
   "outputs": [],
   "source": [
    "def l2_norm(vector):\n",
    "    return np.square(vector)\n",
    "\n",
    "def SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n",
    "    a = l2_norm(denoised)\n",
    "    b = l2_norm(denoised - cleaned)\n",
    "    a_b = a / b\n",
    "    return np.mean(10 * np.log10(a_b + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcPAuMZ9SlHa"
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, baseline=None)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='./denoiser_cnn_log_mel_generator.h5', \n",
    "                                                         monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model.fit(train_dataset,\n",
    "         steps_per_epoch=600, # you might need to change this\n",
    "         validation_data=test_dataset,\n",
    "         epochs=400,\n",
    "         callbacks=[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1ZZskDZ5L2a"
   },
   "outputs": [],
   "source": [
    "val_loss = model.evaluate(test_dataset)[0]\n",
    "if val_loss < baseline_val_loss:\n",
    "  print(\"New model saved.\")\n",
    "  model.save('./denoiser_cnn_log_mel_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeJTsGxCSuhm"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLiTiK8blE0n"
   },
   "outputs": [],
   "source": [
    "def read_audio(filepath, sample_rate, normalize=True):\n",
    "    \"\"\"Read an audio file and return it as a numpy array\"\"\"\n",
    "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
    "    if normalize:\n",
    "      div_fac = 1 / np.max(np.abs(audio)) / 3.0\n",
    "      audio = audio * div_fac\n",
    "    return audio, sr\n",
    "        \n",
    "def add_noise_to_clean_audio(clean_audio, noise_signal):\n",
    "    \"\"\"Adds noise to an audio sample\"\"\"\n",
    "    if len(clean_audio) >= len(noise_signal):\n",
    "        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n",
    "        while len(clean_audio) >= len(noise_signal):\n",
    "            noise_signal = np.append(noise_signal, noise_signal)\n",
    "\n",
    "    ## Extract a noise segment from a random location in the noise file\n",
    "    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
    "\n",
    "    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n",
    "\n",
    "    speech_power = np.sum(clean_audio ** 2)\n",
    "    noise_power = np.sum(noiseSegment ** 2)\n",
    "    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n",
    "    return noisyAudio\n",
    "\n",
    "def play(audio, sample_rate):\n",
    "    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM6ajbBFlx3b"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n",
    "        self.audio = audio\n",
    "        self.ffT_length = windowLength\n",
    "        self.window_length = windowLength\n",
    "        self.overlap = overlap\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window = scipy.signal.hamming(self.window_length, sym=False)\n",
    "\n",
    "    def get_stft_spectrogram(self):\n",
    "        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n",
    "                            window=self.window, center=True)\n",
    "\n",
    "    def get_audio_from_stft_spectrogram(self, stft_features):\n",
    "        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n",
    "                             window=self.window, center=True)\n",
    "\n",
    "    def get_mel_spectrogram(self):\n",
    "        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n",
    "                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n",
    "\n",
    "    def get_audio_from_mel_spectrogram(self, M):\n",
    "        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n",
    "                                             win_length=self.window_length, window=self.window,\n",
    "                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dcnyvquSoLs"
   },
   "outputs": [],
   "source": [
    "cleanAudio, sr = read_audio(os.path.join(mozilla_basepath, 'test', 'common_voice_en_16526.mp3'), sample_rate=fs)\n",
    "print(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\n",
    "ipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaHe1okPTvV-"
   },
   "outputs": [],
   "source": [
    "noiseAudio, sr = read_audio(os.path.join(UrbanSound8K_basepath, 'test', '7913-3-0-0.wav'), sample_rate=fs)\n",
    "print(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\n",
    "ipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bu_eOKRfTHbp"
   },
   "outputs": [],
   "source": [
    "cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
    "stft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\n",
    "stft_features = np.abs(stft_features)\n",
    "print(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHWcmobyTP4E"
   },
   "outputs": [],
   "source": [
    "noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\n",
    "ipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75M29dl3bBeF"
   },
   "outputs": [],
   "source": [
    "def prepare_input_features(stft_features):\n",
    "    # Phase Aware Scaling: To avoid extreme differences (more than\n",
    "    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n",
    "    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n",
    "    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n",
    "\n",
    "    for index in range(noisySTFT.shape[1] - numSegments + 1):\n",
    "        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n",
    "    return stftSegments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cjO5-cjTP6t"
   },
   "outputs": [],
   "source": [
    "noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
    "noise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n",
    "\n",
    "# Paper: Besides, spectral phase was not used in the training phase.\n",
    "# At reconstruction, noisy spectral phase was used instead to\n",
    "# perform in- verse STFT and recover human speech.\n",
    "noisyPhase = np.angle(noise_stft_features)\n",
    "print(noisyPhase.shape)\n",
    "noise_stft_features = np.abs(noise_stft_features)\n",
    "\n",
    "mean = np.mean(noise_stft_features)\n",
    "std = np.std(noise_stft_features)\n",
    "noise_stft_features = (noise_stft_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3p5PWWkrlE3m"
   },
   "outputs": [],
   "source": [
    "predictors = prepare_input_features(noise_stft_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pSoOw2fTP9N"
   },
   "outputs": [],
   "source": [
    "predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\n",
    "predictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\n",
    "print('predictors.shape:', predictors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5sNR4sSTP_1"
   },
   "outputs": [],
   "source": [
    "STFTFullyConvolutional = model.predict(predictors)\n",
    "print(STFTFullyConvolutional.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzCga3PdUVwG"
   },
   "outputs": [],
   "source": [
    "def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n",
    "    # scale the outpus back to the original range\n",
    "    if cleanMean and cleanStd:\n",
    "        features = cleanStd * features + cleanMean\n",
    "\n",
    "    phase = np.transpose(phase, (1, 0))\n",
    "    features = np.squeeze(features)\n",
    "\n",
    "    # features = librosa.db_to_power(features)\n",
    "    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n",
    "\n",
    "    features = np.transpose(features, (1, 0))\n",
    "    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWlUDtPzURlQ"
   },
   "outputs": [],
   "source": [
    "denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\n",
    "print(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\n",
    "ipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvEIBy7EHgeP"
   },
   "outputs": [],
   "source": [
    "# A numeric identifier of the sound class -- Types of noise\n",
    "# 0 = air_conditioner\n",
    "# 1 = car_horn\n",
    "# 2 = children_playing\n",
    "# 3 = dog_bark\n",
    "# 4 = drilling\n",
    "# 5 = engine_idling\n",
    "# 6 = gun_shot\n",
    "# 7 = jackhammer\n",
    "# 8 = siren\n",
    "# 9 = street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv_7ZwWaUW0_"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n",
    "\n",
    "ax1.plot(cleanAudio)\n",
    "ax1.set_title(\"Clean Audio\")\n",
    "\n",
    "ax2.plot(noisyAudio)\n",
    "ax2.set_title(\"Noisy Audio\")\n",
    "\n",
    "ax3.plot(denoisedAudioFullyConvolutional)\n",
    "ax3.set_title(\"Denoised Audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LM7E91avKA3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SpeechDenoiserCNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cuda37",
   "language": "python",
   "name": "cuda37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
